# ============================================================================
# FICHIER : app/train_model.py
# ============================================================================
#
# QU'EST-CE QUE CE SCRIPT ?
# =========================
# Ce script entra√Æne un mod√®le de machine learning (XGBoost) pour pr√©dire l'attrition
# des employ√©s. L'attrition = d√©part d'un employ√© de l'entreprise.
#
# POURQUOI XGBOOST ?
# ===================
# XGBoost (eXtreme Gradient Boosting) est un algorithme tr√®s performant pour :
# - La classification binaire (l'employ√© reste ou part)
# - Les donn√©es tabulaires (tableaux avec colonnes)
# - La gestion des valeurs manquantes
#
# PIPELINE COMPLET :
# ==================
# 1. Charger les donn√©es depuis PostgreSQL (3 tables : sirh, eval, sondage)
# 2. Pr√©parer les donn√©es (nettoyage, encodage, features d√©riv√©es)
# 3. Optimiser les hyperparam√®tres avec Optuna (recherche automatique)
# 4. Entra√Æner le mod√®le final avec les meilleurs hyperparam√®tres
# 5. √âvaluer le mod√®le sur un jeu de test
# 6. Sauvegarder le mod√®le et tous les artefacts n√©cessaires
#
# VARIABLES D'ENVIRONNEMENT :
# ============================
# - DATABASE_URL : URL de connexion PostgreSQL
#   Exemple : postgresql+psycopg2://user:pwd@host:5432/db
# - OPTUNA_TRIALS : Nombre d'essais Optuna (d√©faut : 60)
#   Plus d'essais = meilleur mod√®le mais plus long
# - OPTUNA_TIMEOUT : Temps maximum en secondes (d√©faut : None = pas de limite)
#   Exemple : 1800 = 30 minutes maximum
#
# ============================================================================

"""
Script d'entra√Ænement du mod√®le XGBoost (classification binaire, AUC)

Ce script fait :
  1) Lecture des donn√©es PostgreSQL (SQLAlchemy)
  2) Pr√©paration des donn√©es (encodages, features d√©riv√©es)
  3) Optimisation des hyperparam√®tres avec OPTUNA (TPE)
  4) Entra√Ænement final avec early stopping (via xgboost.train)
  5) √âvaluation sur le test + Sauvegarde des artefacts

Variables d'environnement utiles :
  - DATABASE_URL     : ex. postgresql+psycopg2://user:pwd@host:5432/db
  - OPTUNA_TRIALS    : nb d'essais Optuna (ex. 60 ; d√©faut=60)
  - OPTUNA_TIMEOUT   : temps max en secondes (ex. 1800 ; d√©faut=None)
"""

# ============================================================================
# IMPORTS
# ============================================================================
import os
import sys
import warnings
from pathlib import Path
from typing import Tuple, Optional, Dict, Any

# joblib : pour sauvegarder/charger les mod√®les et pr√©processeurs
import joblib

# numpy : calculs num√©riques (tableaux multidimensionnels)
import numpy as np

# pandas : manipulation de donn√©es tabulaires (DataFrames)
import pandas as pd

# sqlalchemy : ORM pour interagir avec PostgreSQL
from sqlalchemy import create_engine, text

# sklearn.metrics : m√©triques pour √©valuer le mod√®le
from sklearn.metrics import (
    classification_report,      # Rapport d√©taill√© (pr√©cision, rappel, F1)
    roc_auc_score,              # AUC-ROC (aire sous la courbe ROC)
    average_precision_score,    # AUC-PR (aire sous la courbe pr√©cision-rappel)
)

# sklearn.model_selection : outils pour diviser les donn√©es
from sklearn.model_selection import train_test_split

# sklearn.preprocessing : outils de pr√©paration des donn√©es
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder

# xgboost : biblioth√®que de machine learning (gradient boosting)
# On utilise l'API bas niveau xgboost.train (compatible avec les versions plus anciennes)
import xgboost as xgb

# optuna : biblioth√®que d'optimisation d'hyperparam√®tres
# TPE = Tree-structured Parzen Estimator (algorithme de recherche)
# On √©vite les callbacks d'int√©gration pour compatibilit√© maximale
import optuna
from optuna.samplers import TPESampler      # Algorithme de recherche
from optuna.pruners import MedianPruner    # Arr√™t pr√©matur√© des essais peu prometteurs


# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

def safe_divide(num, den, fill_value: float = 0.0) -> pd.Series:
    """
    Division s√ªre avec gestion des NaN/infini et division par z√©ro.
    
    QU'EST-CE QU'UNE DIVISION S√õRE ?
    =================================
    En math√©matiques, diviser par z√©ro est impossible (erreur).
    En programmation, cela cr√©e des valeurs infinies (inf) ou NaN (Not a Number).
    Cette fonction √©vite ces probl√®mes.
    
    COMMENT √áA MARCHE ?
    ===================
    1. Convertit les valeurs en nombres (g√®re les erreurs)
    2. V√©rifie que le d√©nominateur est > 0 et fini
    3. Divise seulement si c'est s√ªr
    4. Remplace les NaN/inf par une valeur par d√©faut (fill_value)
    
    PARAM√àTRES :
    ===========
    - num : num√©rateur (peut √™tre un nombre, une s√©rie pandas, etc.)
    - den : d√©nominateur (peut √™tre un nombre, une s√©rie pandas, etc.)
    - fill_value : valeur √† utiliser si division impossible (d√©faut : 0.0)
    
    RETOUR :
    ========
    pd.Series : s√©rie pandas avec les r√©sultats de la division
    
    EXEMPLE :
    =========
    >>> safe_divide([10, 20, 30], [2, 0, 5], fill_value=0.0)
    [5.0, 0.0, 6.0]  # 20/0 devient 0.0 au lieu d'inf
    """
    # Convertir en nombres (errors="coerce" = remplace les erreurs par NaN)
    num = pd.to_numeric(num, errors="coerce")
    den = pd.to_numeric(den, errors="coerce")
    
    # Division conditionnelle :
    # - Si den > 0 ET den est fini (pas inf), alors num / den
    # - Sinon, NaN
    # np.where : √©quivalent d'un if/else pour chaque √©l√©ment
    out = np.where((den > 0) & np.isfinite(den), num / den, np.nan)
    
    # Convertir en s√©rie pandas (conserve l'index si num est une s√©rie)
    out = pd.Series(out, index=num.index if isinstance(num, pd.Series) else None)
    
    # Remplacer les NaN par fill_value (0.0 par d√©faut)
    return out.fillna(fill_value)


# ============================================================================
# CHARGEMENT DES DONN√âES DEPUIS POSTGRESQL
# ============================================================================

def load_data_from_db() -> pd.DataFrame:
    """
    Charge les tables sirh, eval (ou performance), sondage et joint sur id_employee.
    
    QU'EST-CE QU'UNE JOINTURE ?
    ===========================
    Une jointure combine plusieurs tables en une seule.
    Ici, on a 3 tables avec des informations diff√©rentes sur les employ√©s :
    - sirh : informations RH (√¢ge, salaire, d√©partement, etc.)
    - eval : √©valuations de performance
    - sondage : r√©sultats de sondages
    
    Toutes ces tables sont li√©es par "id_employee" (identifiant unique).
    
    COMMENT √áA MARCHE ?
    ===================
    1. Se connecter √† PostgreSQL avec SQLAlchemy
    2. Charger chaque table s√©par√©ment avec pd.read_sql()
    3. Harmoniser les cl√©s (s'assurer que id_employee existe partout)
    4. Faire des jointures gauches (LEFT JOIN) pour combiner les tables
    5. Retourner un DataFrame unique avec toutes les colonnes
    
    RETOUR :
    ========
    pd.DataFrame : DataFrame pandas avec toutes les donn√©es combin√©es
    
    EXEMPLE DE STRUCTURE :
    ======================
    id_employee | age | revenu_mensuel | note_evaluation | a_quitte_l_entreprise
    1           | 35  | 5000          | 4.5             | Non
    2           | 28  | 3500          | 3.8             | Oui
    ...
    """
    # R√©cup√©rer l'URL de la base de donn√©es depuis les variables d'environnement
    # Si DATABASE_URL n'existe pas, utiliser une valeur par d√©faut (localhost)
    DATABASE_URL = os.getenv(
        "DATABASE_URL",
        "postgresql+psycopg2://postgres:password@localhost:5432/mydatabase",
    )
    
    # Cr√©er un moteur SQLAlchemy pour se connecter √† PostgreSQL
    # pool_pre_ping=True : v√©rifie que les connexions sont valides avant utilisation
    # future=True : utilise l'API moderne de SQLAlchemy
    engine = create_engine(DATABASE_URL, pool_pre_ping=True, future=True)

    # Ouvrir une connexion et charger les 3 tables
    # engine.begin() : cr√©e une transaction (toutes les op√©rations r√©ussissent ou √©chouent ensemble)
    with engine.begin() as conn:
        # Charger la table SIRH (Syst√®me d'Information des Ressources Humaines)
        # text('SELECT * FROM "sirh";') : requ√™te SQL brute
        # Les guillemets doubles sont n√©cessaires car "sirh" est en minuscules
        sirh = pd.read_sql(text('SELECT * FROM "sirh";'), conn)
        
        # Charger la table eval (√©valuations)
        # Note : Si votre table s'appelle "performance", remplacez "eval" par "performance"
        eval_df = pd.read_sql(text('SELECT * FROM "eval";'), conn)
        
        # Charger la table sondage (r√©sultats de sondages)
        sondage = pd.read_sql(text('SELECT * FROM "sondage";'), conn)

    # ========================================================================
    # HARMONISATION DES CL√âS
    # ========================================================================
    # Probl√®me : les 3 tables peuvent avoir des formats diff√©rents pour id_employee
    # Solution : convertir toutes les cl√©s au m√™me format (entier)
    
    # Table SIRH : id_employee devrait d√©j√† √™tre un entier
    if "id_employee" in sirh.columns:
        # pd.to_numeric : convertit en nombre (errors="coerce" = NaN si erreur)
        # .astype("Int64") : convertit en entier (Int64 permet les NaN)
        sirh["id_employee"] = pd.to_numeric(sirh["id_employee"], errors="coerce").astype("Int64")

    # Table eval : id_employee peut √™tre dans "eval_number" avec format "E_23"
    # On extrait le nombre apr√®s "E_" (ex: "E_23" -> 23)
    if "eval_number" in eval_df.columns:
        # .astype(str) : convertit en cha√Æne
        # .str[2:] : prend tout apr√®s les 2 premiers caract√®res ("E_" -> reste "23")
        # pd.to_numeric : convertit "23" en nombre 23
        eval_df["id_employee"] = pd.to_numeric(eval_df["eval_number"].astype(str).str[2:], errors="coerce").astype("Int64")

    # Table sondage : id_employee peut √™tre dans "code_sondage"
    if "code_sondage" in sondage.columns:
        sondage["id_employee"] = pd.to_numeric(sondage["code_sondage"], errors="coerce").astype("Int64")

    # ========================================================================
    # JOINTURES GAUCHES (LEFT JOIN)
    # ========================================================================
    # LEFT JOIN : garde toutes les lignes de la table de gauche (sirh)
    # et ajoute les colonnes des autres tables si elles existent
    
    # Premi√®re jointure : sirh + eval
    df = sirh.merge(
        eval_df.drop(columns=["eval_number"], errors="ignore"),  # Supprimer eval_number (plus besoin)
        on="id_employee",      # Cl√© de jointure
        how="left",            # LEFT JOIN (garde tous les employ√©s de sirh)
    ).merge(
        # Deuxi√®me jointure : r√©sultat pr√©c√©dent + sondage
        sondage.drop(columns=["code_sondage"], errors="ignore"),  # Supprimer code_sondage
        on="id_employee",
        how="left",
    )
    
    return df


# ============================================================================
# PR√âPARATION DES DONN√âES
# ============================================================================

def prepare_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[OneHotEncoder], Optional[OrdinalEncoder]]:
    """
    Pr√©pare les donn√©es pour l'entra√Ænement du mod√®le.
    
    QU'EST-CE QUE LA PR√âPARATION DES DONN√âES ?
    ===========================================
    Les mod√®les de machine learning ne peuvent pas travailler directement avec
    des donn√©es brutes. Il faut :
    1. Nettoyer les donn√©es (corriger les erreurs, g√©rer les valeurs manquantes)
    2. Encoder les variables cat√©gorielles (texte -> nombres)
    3. Cr√©er des features d√©riv√©es (nouvelles colonnes calcul√©es)
    4. Supprimer les colonnes inutiles
    
    PIPELINE DE PR√âPARATION :
    =========================
    a) Normalisations/corrections de colonnes
    b) Cr√©ation de la variable cible (Attrition)
    c) Recodage oui/non -> 1/0
    d) Encodages cat√©goriels (OneHot + Ordinal)
    e) Dummies de secours (pour les cat√©gories restantes)
    f) Conversion bool -> int
    g) Features d√©riv√©es (ratios, √©carts, etc.)
    h) Suppression de colonnes corr√©l√©es
    i) Nettoyage final (NaN, infini)
    
    RETOUR :
    ========
    Tuple contenant :
    - df_model : DataFrame pr√©par√© (pr√™t pour l'entra√Ænement)
    - ohe : Encodeur OneHot (pour r√©utiliser en production)
    - ordinal_encoder : Encodeur ordinal (pour r√©utiliser en production)
    """
    # Cr√©er une copie pour ne pas modifier le DataFrame original
    df_ = df.copy()

    # ========================================================================
    # a) NORMALISATIONS / CORRECTIONS DE COLONNES
    # ========================================================================
    
    # Probl√®me : le nom de la colonne peut √™tre mal orthographi√©
    # Solution : chercher les deux variantes possibles
    col_aug_misspelled = "augementation_salaire_precedente"  # Faute d'orthographe
    col_aug = "augmentation_salaire_precedente"              # Correct
    
    # Chercher quelle colonne existe
    src_col = col_aug_misspelled if col_aug_misspelled in df_.columns else (col_aug if col_aug in df_.columns else None)
    
    if src_col:
        # Normaliser le format de l'augmentation salariale
        # Exemple : "11%" ou "11,5%" -> 0.11 ou 0.115
        df_["augmentation_taux"] = (
            df_[src_col].astype(str)                    # Convertir en cha√Æne
            .str.replace("%", "", regex=False)            # Supprimer le %
            .str.replace(",", ".", regex=False)           # Remplacer , par .
            .str.replace(r"\s+", "", regex=True)          # Supprimer les espaces
            .pipe(pd.to_numeric, errors="coerce")         # Convertir en nombre
            .div(100)                                      # Diviser par 100 (11% -> 0.11)
        )
        # Supprimer l'ancienne colonne et renommer la nouvelle
        df_.drop(columns=[src_col], inplace=True)
        df_.rename(columns={"augmentation_taux": col_aug}, inplace=True)

    # Corriger le nom de la colonne (faute d'orthographe dans la base)
    if "nombre_heures_travailless" in df_.columns:
        df_.rename(columns={"nombre_heures_travailless": "nombre_heures_travaillees"}, inplace=True)

    # ========================================================================
    # b) CR√âATION DE LA VARIABLE CIBLE (ATTRITION)
    # ========================================================================
    # La variable cible = ce qu'on veut pr√©dire
    # Ici : l'employ√© a-t-il quitt√© l'entreprise ? (Oui = 1, Non = 0)
    
    if "a_quitte_l_entreprise" in df_.columns:
        df_["Attrition"] = (
            df_["a_quitte_l_entreprise"].astype(str)      # Convertir en cha√Æne
            .str.strip()                                   # Supprimer les espaces
            .str.lower()                                   # Mettre en minuscules
            .map({"oui": 1, "non": 0})                    # Mapper oui->1, non->0
            .fillna(0)                                     # Remplacer NaN par 0
            .astype(int)                                   # Convertir en entier
        )

    # Supprimer les colonnes inutiles pour le mod√®le
    df_.drop(
        columns=[
            "a_quitte_l_entreprise",              # D√©j√† converti en Attrition
            "nombre_heures_travaillees",          # Non utilis√© dans le mod√®le
            "id_employee",                        # Identifiant (pas une feature)
            "ayant_enfants",                      # Non utilis√©
            "nombre_employee_sous_responsabilite", # Non utilis√©
        ],
        errors="ignore",  # Ignorer si la colonne n'existe pas
        inplace=True,
    )

    # ========================================================================
    # c) RECODAGE OUI/NON -> 1/0
    # ========================================================================
    # Les mod√®les pr√©f√®rent les nombres (0/1) plut√¥t que les cha√Ænes ("Oui"/"Non")
    
    if "heure_supplementaires" in df_.columns and df_["heure_supplementaires"].dtype == "object":
        df_["heure_supplementaires"] = (
            df_["heure_supplementaires"].astype(str)      # Convertir en cha√Æne
            .str.strip()                                   # Supprimer les espaces
            .str.lower()                                   # Mettre en minuscules
            .map({"oui": 1, "non": 0})                    # Mapper oui->1, non->0
            .astype("Int64")                               # Convertir en entier (avec NaN support)
        )

    # ========================================================================
    # d) ENCODAGES CAT√âGORIELS
    # ========================================================================
    # Probl√®me : les mod√®les ne comprennent pas le texte ("R&D", "Commercial", etc.)
    # Solution : convertir en nombres avec des encodages
    
    # Colonnes nominales (sans ordre) : d√©partement, poste, domaine_etude, statut_marital
    colonnes_nominales = [c for c in ["departement", "poste", "domaine_etude", "statut_marital"] if c in df_.columns]
    
    # OneHotEncoder : cr√©e une colonne par cat√©gorie
    # Exemple : d√©partement "R&D" -> colonne "departement_R&D" = 1, autres = 0
    # drop="first" : supprime la premi√®re colonne (√©vite la colin√©arit√©)
    # sparse_output=False : retourne un tableau dense (pas une matrice creuse)
    # handle_unknown="ignore" : si une nouvelle cat√©gorie appara√Æt, la traite comme z√©ros
    try:
        ohe = OneHotEncoder(drop="first", sparse_output=False, handle_unknown="ignore")
    except TypeError:
        # Compatibilit√© avec les anciennes versions de scikit-learn
        ohe = OneHotEncoder(drop="first", sparse=False, handle_unknown="ignore")

    if colonnes_nominales:
        # Appliquer l'encodage OneHot
        # fit_transform : apprend les cat√©gories ET transforme les donn√©es
        df_ohe = pd.DataFrame(
            ohe.fit_transform(df_[colonnes_nominales]),    # Encodage
            columns=ohe.get_feature_names_out(colonnes_nominales),  # Noms des colonnes
            index=df_.index,                               # Conserver l'index
        )
        # Supprimer les colonnes originales et ajouter les colonnes encod√©es
        df_.drop(columns=colonnes_nominales, inplace=True)
        df_ = pd.concat([df_, df_ohe], axis=1)
    else:
        ohe = None  # Pas d'encodage si pas de colonnes nominales

    # OrdinalEncoder : pour les colonnes avec un ordre naturel
    # Exemple : "Aucun" < "Occasionnel" < "Frequent"
    ordinal_encoder = None
    if "frequence_deplacement" in df_.columns:
        # categories : d√©finit l'ordre des cat√©gories
        ordinal_encoder = OrdinalEncoder(categories=[["Aucun", "Occasionnel", "Frequent"]])
        # fit_transform : apprend l'ordre ET transforme
        df_[["frequence_deplacement"]] = ordinal_encoder.fit_transform(df_[["frequence_deplacement"]])

    # ========================================================================
    # e) DUMMIES DE SECOURS
    # ========================================================================
    # Pour les colonnes cat√©gorielles restantes (non encod√©es pr√©c√©demment)
    # pd.get_dummies : cr√©e des colonnes binaires (0/1) pour chaque cat√©gorie
    # drop_first=True : supprime la premi√®re colonne (√©vite la colin√©arit√©)
    df_model = pd.get_dummies(df_, drop_first=True)

    # ========================================================================
    # f) CONVERSION BOOL -> INT
    # ========================================================================
    # Les colonnes bool√©ennes (True/False) doivent √™tre converties en 1/0
    bool_cols = df_model.select_dtypes(include="bool").columns.tolist()
    if bool_cols:
        df_model[bool_cols] = df_model[bool_cols].astype(int)

    # ========================================================================
    # g) FEATURES D√âRIV√âES
    # ========================================================================
    # Cr√©er de nouvelles colonnes calcul√©es √† partir des colonnes existantes
    # Ces features peuvent am√©liorer les performances du mod√®le
    
    new_cols: Dict[str, pd.Series] = {}  # Dictionnaire pour stocker les nouvelles colonnes

    # Fonction helper pour cr√©er des ratios
    def make_ratio(dfm: pd.DataFrame, num: str, den: str) -> pd.Series:
        """Cr√©e un ratio (num√©rateur / d√©nominateur) de mani√®re s√ªre."""
        if num in dfm.columns and den in dfm.columns:
            return safe_divide(dfm[num], dfm[den], fill_value=0.0)
        return pd.Series(0.0, index=dfm.index)

    # Ratio d'anciennet√© : ann√©es dans l'entreprise / ann√©es d'exp√©rience totale
    # Plus proche de 1 = l'employ√© a pass√© toute sa carri√®re dans l'entreprise
    new_cols["ratio_anciennete"] = make_ratio(df_model, "annees_dans_l_entreprise", "annee_experience_totale")
    
    # Ratio poste : ann√©es dans le poste actuel / ann√©es dans l'entreprise
    # Plus proche de 1 = l'employ√© est dans le m√™me poste depuis longtemps
    new_cols["ratio_poste"] = make_ratio(df_model, "annees_dans_le_poste_actuel", "annees_dans_l_entreprise")
    
    # Ratio formations : nombre de formations / ann√©es dans l'entreprise
    # Plus √©lev√© = l'employ√© se forme beaucoup
    new_cols["ratio_formations"] = make_ratio(df_model, "nb_formations_suivies", "annees_dans_l_entreprise")

    # √âcart d'√©valuation : note actuelle - note pr√©c√©dente
    # Positif = am√©lioration, n√©gatif = d√©t√©rioration
    if {"note_evaluation_actuelle", "note_evaluation_precedente"}.issubset(df_model.columns):
        new_cols["ecart_evaluation"] = (
            df_model["note_evaluation_actuelle"] - df_model["note_evaluation_precedente"]
        ).fillna(0.0)
        # Supprimer la note pr√©c√©dente (d√©j√† utilis√©e dans l'√©cart)
        df_model = df_model.drop(columns="note_evaluation_precedente", errors="ignore")
    else:
        new_cols["ecart_evaluation"] = pd.Series(0.0, index=df_model.index)

    # Ratio salaire/niveau : revenu mensuel / niveau hi√©rarchique
    # Plus √©lev√© = mieux pay√© par rapport √† son niveau
    salaire_col = next((c for c in ["revenu_mensuel", "salaire_mensuel", "MonthlyIncome"] if c in df_model.columns), None)
    niveau_col = next((c for c in ["niveau_hierarchique_poste", "JobLevel"] if c in df_model.columns), None)
    if salaire_col and niveau_col:
        new_cols["ratio_salaire_niveau"] = make_ratio(df_model, salaire_col, niveau_col)
    else:
        new_cols["ratio_salaire_niveau"] = pd.Series(0.0, index=df_model.index)

    # Indice de promotion r√©cente : 1 / (ann√©es depuis promotion + 1)
    # Plus proche de 1 = promotion tr√®s r√©cente
    if "annees_depuis_la_derniere_promotion" in df_model.columns:
        new_cols["indice_recente_promo"] = 1.0 / (df_model["annees_depuis_la_derniere_promotion"].fillna(0) + 1.0)
        df_model = df_model.drop(columns="annees_depuis_la_derniere_promotion", errors="ignore")
    else:
        new_cols["indice_recente_promo"] = pd.Series(0.0, index=df_model.index)

    # Ajouter toutes les nouvelles colonnes au DataFrame
    df_model = pd.concat([df_model, pd.DataFrame(new_cols)], axis=1)

    # ========================================================================
    # h) SUPPRESSION DE COLONNES CORR√âL√âES
    # ========================================================================
    # Certaines colonnes sont tr√®s corr√©l√©es (redondantes)
    # Les supprimer √©vite le surapprentissage (overfitting)
    colonnes_a_supprimer = [
        "annees_dans_le_poste_actuel",      # D√©j√† utilis√© dans ratio_poste
        "annes_sous_responsable_actuel",     # Peu informatif
        "poste_Ressources Humaines",         # Cat√©gorie rare
        "annee_experience_totale",          # D√©j√† utilis√© dans ratio_anciennete
        "niveau_hierarchique_poste",         # D√©j√† utilis√© dans ratio_salaire_niveau
    ]
    df_model.drop(columns=[c for c in colonnes_a_supprimer if c in df_model.columns], inplace=True, errors="ignore")

    # ========================================================================
    # i) NETTOYAGE FINAL
    # ========================================================================
    # Remplacer les valeurs infinies et NaN par 0.0
    # Les mod√®les ne peuvent pas g√©rer infini ou NaN
    df_model.replace([np.inf, -np.inf], np.nan, inplace=True)
    df_model.fillna(0.0, inplace=True)

    return df_model, ohe, ordinal_encoder


# ============================================================================
# OPTIMISATION DES HYPERPARAM√àTRES AVEC OPTUNA
# ============================================================================

def _compute_scale_pos_weight(y: pd.Series) -> float:
    """
    Calcule le poids pour √©quilibrer les classes (d√©s√©quilibre positif/n√©gatif).
    
    QU'EST-CE QUE LE D√âS√âQUILIBRE DE CLASSES ?
    ==========================================
    Si on a 90% d'employ√©s qui restent et 10% qui partent, le mod√®le peut
    toujours pr√©dire "reste" et avoir 90% de pr√©cision (mais inutile !).
    
    scale_pos_weight permet de donner plus d'importance aux exemples positifs (d√©part).
    
    RETOUR :
    ========
    float : ratio (#n√©gatifs / #positifs)
    Exemple : si 90% restent et 10% partent, retourne 9.0
    """
    pos = int((y == 1).sum())  # Nombre d'employ√©s qui partent
    neg = int((y == 0).sum())  # Nombre d'employ√©s qui restent
    # Retourner le ratio, minimum 1.0 (pas de d√©s√©quilibre si pos = 0)
    return max(1.0, neg / pos) if pos > 0 else 1.0


def _optuna_objective_factory(
    X_tr: pd.DataFrame,
    y_tr: pd.Series,
    X_val: pd.DataFrame,
    y_val: pd.Series,
    base_params: Dict[str, Any],
):
    """
    Construit la fonction objective pour Optuna.
    
    QU'EST-CE QU'UNE FONCTION OBJECTIVE ?
    =====================================
    C'est la fonction qu'Optuna essaie d'optimiser (maximiser).
    Ici, on maximise l'AUC (Area Under Curve) sur le jeu de validation.
    
    COMMENT √áA MARCHE ?
    ===================
    1. Optuna propose des hyperparam√®tres (learning_rate, max_depth, etc.)
    2. On entra√Æne un mod√®le avec ces hyperparam√®tres
    3. On calcule l'AUC sur le jeu de validation
    4. On retourne l'AUC (Optuna cherche √† la maximiser)
    
    PARAM√àTRES :
    ===========
    - X_tr, y_tr : donn√©es d'entra√Ænement (features et cible)
    - X_val, y_val : donn√©es de validation (pour √©valuer les hyperparam√®tres)
    - base_params : param√®tres de base (objectif, m√©trique, etc.)
    
    RETOUR :
    ========
    function : fonction objective(trial) que Optuna peut appeler
    """
    
    # Conversion en DMatrix (format natif XGBoost)
    # DMatrix est optimis√© pour XGBoost (plus rapide que DataFrame)
    dtrain = xgb.DMatrix(X_tr.values, label=y_tr.values)
    dvalid = xgb.DMatrix(X_val.values, label=y_val.values)

    def objective(trial: optuna.trial.Trial) -> float:
        """
        Fonction objective appel√©e par Optuna pour chaque essai.
        
        PARAM√àTRES :
        ============
        - trial : objet Optuna qui propose des hyperparam√®tres √† tester
        
        RETOUR :
        ========
        float : AUC sur le jeu de validation (√† maximiser)
        """
        # ====================================================================
        # ESPACE DE RECHERCHE DES HYPERPARAM√àTRES
        # ====================================================================
        # Optuna propose des valeurs pour chaque hyperparam√®tre
        # trial.suggest_* : demande √† Optuna de proposer une valeur
        
        params = {
            **base_params,  # Param√®tres de base (objectif, m√©trique, etc.)
            
            # learning_rate (alias "eta") : vitesse d'apprentissage
            # Plus petit = apprentissage plus lent mais plus stable
            # log=True : recherche sur √©chelle logarithmique (0.001 √† 0.3)
            "eta": trial.suggest_float("learning_rate", 1e-3, 0.3, log=True),
            
            # max_depth : profondeur maximale des arbres
            # Plus profond = mod√®le plus complexe (risque de surapprentissage)
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            
            # min_child_weight : poids minimum des feuilles
            # Plus √©lev√© = mod√®le plus simple (√©vite le surapprentissage)
            "min_child_weight": trial.suggest_float("min_child_weight", 1.0, 10.0),
            
            # subsample : proportion d'√©chantillons utilis√©s pour chaque arbre
            # 0.5 = utilise 50% des donn√©es (r√©duit le surapprentissage)
            "subsample": trial.suggest_float("subsample", 0.5, 1.0),
            
            # colsample_bytree : proportion de features utilis√©es pour chaque arbre
            # 0.5 = utilise 50% des colonnes (r√©duit le surapprentissage)
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
            
            # lambda (reg_lambda) : r√©gularisation L2
            # Plus √©lev√© = mod√®le plus simple (√©vite le surapprentissage)
            "lambda": trial.suggest_float("reg_lambda", 1e-3, 100, log=True),
            
            # alpha (reg_alpha) : r√©gularisation L1
            # Plus √©lev√© = mod√®le plus simple (s√©lection de features)
            "alpha": trial.suggest_float("reg_alpha", 1e-3, 10, log=True),
            
            # gamma : seuil minimum de gain pour diviser un n≈ìud
            # Plus √©lev√© = mod√®le plus simple (moins de divisions)
            "gamma": trial.suggest_float("gamma", 0.0, 5.0),
        }
        
        # Nombre d'arbres (boosting rounds)
        # Plus d'arbres = mod√®le plus complexe (mais risque de surapprentissage)
        num_boost_round = trial.suggest_int("n_estimators", 300, 1200, step=100)

        # ====================================================================
        # ENTRA√éNEMENT AVEC EARLY STOPPING
        # ====================================================================
        # early_stopping_rounds : arr√™te l'entra√Ænement si AUC ne s'am√©liore pas
        # Exemple : si AUC ne s'am√©liore pas pendant 50 rounds, arr√™ter
        booster = xgb.train(
            params=params,                    # Hyperparam√®tres propos√©s par Optuna
            dtrain=dtrain,                     # Donn√©es d'entra√Ænement
            num_boost_round=num_boost_round,  # Nombre maximum d'arbres
            evals=[(dvalid, "valid")],         # Jeu de validation (pour early stopping)
            early_stopping_rounds=50,          # Patience (50 rounds sans am√©lioration)
            verbose_eval=False,                # Ne pas afficher les logs
        )

        # ====================================================================
        # R√âCUP√âRATION DU MEILLEUR SCORE AUC
        # ====================================================================
        # XGBoost stocke le meilleur score AUC dans booster.best_score
        auc_val = float(getattr(booster, "best_score", np.nan))
        
        if np.isnan(auc_val):
            # Fallback : calculer l'AUC manuellement si best_score n'existe pas
            best_iter = getattr(booster, "best_iteration", None)
            if best_iter is not None:
                # Utiliser le meilleur nombre d'it√©rations trouv√© par early stopping
                proba_val = booster.predict(dvalid, iteration_range=(0, best_iter + 1))
            else:
                # Utiliser toutes les it√©rations
                proba_val = booster.predict(dvalid)
            # Calculer l'AUC avec sklearn
            auc_val = roc_auc_score(y_val.values, proba_val)

        # Retourner l'AUC (Optuna cherche √† maximiser cette valeur)
        return auc_val

    return objective


def tune_with_optuna(
    X_train_full: pd.DataFrame,
    y_train_full: pd.Series,
    n_trials: int = 60,
    timeout: Optional[int] = None,
    seed: int = 1042,
) -> Dict[str, Any]:
    """
    Lance l'optimisation Optuna pour trouver les meilleurs hyperparam√®tres.
    
    QU'EST-CE QU'OPTUNA ?
    =====================
    Optuna est une biblioth√®que d'optimisation d'hyperparam√®tres.
    Elle teste automatiquement diff√©rentes combinaisons pour trouver les meilleures.
    
    ALGORITHME TPE (Tree-structured Parzen Estimator) :
    ===================================================
    - Apprend des essais pr√©c√©dents pour proposer de meilleures valeurs
    - Plus intelligent qu'une recherche al√©atoire ou une grille
    
    PARAM√àTRES :
    ============
    - X_train_full, y_train_full : toutes les donn√©es d'entra√Ænement
    - n_trials : nombre d'essais Optuna (d√©faut : 60)
    - timeout : temps maximum en secondes (d√©faut : None = pas de limite)
    - seed : graine al√©atoire pour la reproductibilit√©
    
    RETOUR :
    ========
    Dict contenant :
    - best_params : meilleurs hyperparam√®tres trouv√©s
    - best_num_boost_round : meilleur nombre d'arbres
    - study : objet Optuna (pour analyse)
    """
    # Diviser les donn√©es d'entra√Ænement en train/validation
    # 15% pour la validation (pour guider early stopping)
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train_full, y_train_full, 
        test_size=0.15,           # 15% pour validation
        random_state=seed,        # Graine pour reproductibilit√©
        stratify=y_train_full     # Conserver la proportion de classes
    )

    # ========================================================================
    # PARAM√àTRES DE BASE (non optimis√©s)
    # ========================================================================
    base_params = {
        "objective": "binary:logistic",  # Classification binaire (logistique)
        "eval_metric": "auc",            # M√©trique √† optimiser (AUC)
        "tree_method": "hist",           # M√©thode de construction d'arbres (rapide)
        # "gpu_hist" si GPU disponible (plus rapide)
        "seed": seed,                    # Graine al√©atoire
        "verbosity": 0,                  # Pas de logs
        "scale_pos_weight": _compute_scale_pos_weight(y_train_full),  # √âquilibrage des classes
    }

    # Cr√©er la fonction objective
    objective = _optuna_objective_factory(X_tr, y_tr, X_val, y_val, base_params)

    # ========================================================================
    # CR√âER L'√âTUDE OPTUNA
    # ========================================================================
    study = optuna.create_study(
        direction="maximize",              # Maximiser l'AUC
        sampler=TPESampler(seed=seed),     # Algorithme TPE
        pruner=MedianPruner(n_warmup_steps=10),  # Arr√™ter les essais peu prometteurs
    )

    # ========================================================================
    # LANCER L'OPTIMISATION
    # ========================================================================
    study.optimize(
        objective,                    # Fonction √† optimiser
        n_trials=n_trials,           # Nombre d'essais
        timeout=timeout,             # Temps maximum
        show_progress_bar=True,      # Afficher la barre de progression
        gc_after_trial=True,         # Nettoyer la m√©moire apr√®s chaque essai
    )

    # Afficher les r√©sultats
    print("üîé Meilleur AUC (val) :", study.best_value)
    print("üîß Meilleurs hyperparam√®tres :", study.best_trial.params)

    # ========================================================================
    # RECOMPOSER LES PARAM√àTRES FINAUX
    # ========================================================================
    # Convertir les hyperparam√®tres d'Optuna au format XGBoost
    best = study.best_trial.params
    best_params = {
        **base_params,
        "eta": best["learning_rate"],        # learning_rate -> eta
        "max_depth": best["max_depth"],
        "min_child_weight": best["min_child_weight"],
        "subsample": best["subsample"],
        "colsample_bytree": best["colsample_bytree"],
        "lambda": best["reg_lambda"],        # reg_lambda -> lambda
        "alpha": best["reg_alpha"],          # reg_alpha -> alpha
        "gamma": best["gamma"],
    }
    best_num_boost_round = best["n_estimators"]

    return {"best_params": best_params, "best_num_boost_round": best_num_boost_round, "study": study}


# ============================================================================
# ENTRA√éNEMENT PRINCIPAL
# ============================================================================

def train_model():
    """
    Pipeline complet d'entra√Ænement du mod√®le.
    
    √âTAPES :
    ========
    1. Charger les donn√©es depuis PostgreSQL
    2. Pr√©parer les donn√©es (nettoyage, encodage, features)
    3. Diviser en train/test (80/20)
    4. Optimiser les hyperparam√®tres avec Optuna
    5. Entra√Æner le mod√®le final avec les meilleurs hyperparam√®tres
    6. √âvaluer sur le jeu de test
    7. Sauvegarder le mod√®le et tous les artefacts
    
    RETOUR :
    ========
    Tuple contenant :
    - booster : mod√®le XGBoost entra√Æn√©
    - ohe : encodeur OneHot (pour la production)
    - ordinal_encoder : encodeur ordinal (pour la production)
    - feature_names : liste des noms de features (ordre important)
    """
    # Ignorer les warnings pour des logs plus propres
    warnings.filterwarnings("ignore", category=UserWarning, module="xgboost")
    warnings.filterwarnings("ignore", category=FutureWarning, module="xgboost")
    warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn")

    # ========================================================================
    # √âTAPE 1 : CHARGER LES DONN√âES
    # ========================================================================
    print("üìä Chargement des donn√©es depuis PostgreSQL...")
    df = load_data_from_db()
    print(f"‚úÖ {len(df)} lignes charg√©es")

    # ========================================================================
    # √âTAPE 2 : PR√âPARER LES DONN√âES
    # ========================================================================
    print("üîß Pr√©paration des donn√©es...")
    df_model, ohe, ordinal_encoder = prepare_data(df)

    # V√©rifier que la variable cible existe
    if "Attrition" not in df_model.columns:
        raise ValueError("La colonne cible 'Attrition' est absente apr√®s pr√©paration des donn√©es.")

    # S√©parer les features (X) et la cible (y)
    X = df_model.drop(columns=["Attrition"])  # Toutes les colonnes sauf Attrition
    y = df_model["Attrition"].astype(int)     # Variable cible (0 ou 1)
    print(f"‚úÖ {X.shape[1]} features pr√©par√©es")

    # ========================================================================
    # √âTAPE 3 : DIVISER EN TRAIN/TEST
    # ========================================================================
    # 80% pour l'entra√Ænement, 20% pour le test final
    # stratify=y : conserve la proportion de classes (√©vite le d√©s√©quilibre)
    X_train_full, X_test, y_train_full, y_test = train_test_split(
        X, y, 
        test_size=0.2,           # 20% pour le test
        random_state=1042,       # Graine pour reproductibilit√©
        stratify=y               # Conserver la proportion de classes
    )

    # ========================================================================
    # √âTAPE 4 : OPTIMISER LES HYPERPARAM√àTRES AVEC OPTUNA
    # ========================================================================
    # R√©cup√©rer les param√®tres depuis les variables d'environnement
    n_trials = int(os.getenv("OPTUNA_TRIALS", "60"))      # Nombre d'essais
    timeout_env = os.getenv("OPTUNA_TIMEOUT", None)       # Timeout
    timeout = int(timeout_env) if timeout_env is not None else None

    print(f"üß™ Lancement d'Optuna (n_trials={n_trials}, timeout={timeout}) ...")
    tune_result = tune_with_optuna(
        X_train_full, y_train_full, 
        n_trials=n_trials, 
        timeout=timeout, 
        seed=1042
    )
    best_params = tune_result["best_params"]
    best_num_boost_round = tune_result["best_num_boost_round"]

    # ========================================================================
    # √âTAPE 5 : ENTRA√éNEMENT FINAL
    # ========================================================================
    # Diviser √† nouveau train_full en train/val pour l'entra√Ænement final
    # (diff√©rent du split Optuna pour √©viter le surapprentissage)
    X_tr, X_val, y_tr, y_val = train_test_split(
        X_train_full, y_train_full, 
        test_size=0.15,           # 15% pour validation
        random_state=2042,        # Graine diff√©rente
        stratify=y_train_full
    )

    # Convertir en DMatrix (format XGBoost)
    dtrain = xgb.DMatrix(X_tr.values, label=y_tr.values)
    dvalid = xgb.DMatrix(X_val.values, label=y_val.values)

    # Entra√Æner le mod√®le final avec les meilleurs hyperparam√®tres
    booster = xgb.train(
        params=best_params,                    # Meilleurs hyperparam√®tres trouv√©s
        dtrain=dtrain,                         # Donn√©es d'entra√Ænement
        num_boost_round=best_num_boost_round,  # Meilleur nombre d'arbres
        evals=[(dvalid, "valid")],             # Jeu de validation
        early_stopping_rounds=50,               # Early stopping
        verbose_eval=False,                     # Pas de logs
    )

    # ========================================================================
    # √âTAPE 6 : √âVALUATION SUR LE JEU DE TEST
    # ========================================================================
    # Convertir le jeu de test en DMatrix
    dtest = xgb.DMatrix(X_test.values, label=y_test.values)
    
    # Faire des pr√©dictions avec le meilleur nombre d'it√©rations
    best_iter = getattr(booster, "best_iteration", None)
    if best_iter is not None:
        # Utiliser le meilleur nombre d'it√©rations (trouv√© par early stopping)
        y_proba = booster.predict(dtest, iteration_range=(0, best_iter + 1))
    else:
        # Utiliser toutes les it√©rations si best_iteration n'existe pas
        y_proba = booster.predict(dtest)
    
    # Convertir les probabilit√©s en pr√©dictions binaires (0 ou 1)
    # Seuil = 0.5 : si proba >= 0.5, pr√©dire 1 (part), sinon 0 (reste)
    y_pred = (y_proba >= 0.5).astype(int)

    # Afficher les m√©triques
    print("\nüìä M√©triques sur le jeu de test :")
    print(classification_report(y_test, y_pred))  # Pr√©cision, rappel, F1
    print(f"ROC-AUC: {roc_auc_score(y_test, y_proba):.4f}")  # AUC-ROC
    print(f"AUC-PR: {average_precision_score(y_test, y_proba):.4f}")  # AUC-PR

    # ========================================================================
    # √âTAPE 7 : SAUVEGARDE DES ARTEFACTS
    # ========================================================================
    # Cr√©er le dossier models/ s'il n'existe pas
    models_dir = Path("models")
    models_dir.mkdir(exist_ok=True)

    # a) Sauvegarder le mod√®le XGBoost au format JSON
    # Format JSON = tr√®s compatible, facile √† charger
    booster_path = models_dir / "xgb_booster.json"
    booster.save_model(str(booster_path))

    # b) Sauvegarder les pr√©processeurs et la liste des features
    # IMPORTANT : ces fichiers sont n√©cessaires pour faire des pr√©dictions en production
    # L'ordre des features doit √™tre exactement le m√™me qu'√† l'entra√Ænement
    joblib.dump(ohe, models_dir / "onehot_encoder.joblib")              # Encodeur OneHot
    joblib.dump(ordinal_encoder, models_dir / "ordinal_encoder.joblib")  # Encodeur ordinal
    feature_names = list(X.columns)                                      # Liste des features (ordre)
    joblib.dump(feature_names, models_dir / "feature_names.joblib")

    # c) Sauvegarder les meilleurs hyperparam√®tres (pour audit/reproductibilit√©)
    joblib.dump(
        {"best_params": best_params, "best_num_boost_round": best_num_boost_round},
        models_dir / "xgb_best_params.joblib",
    )

    # Afficher un r√©sum√©
    print(f"\n‚úÖ Mod√®le XGBoost sauvegard√© : {booster_path}")
    print(f"‚úÖ {len(feature_names)} features sauvegard√©es")
    print(f"‚úÖ Hyperparam√®tres Optuna sauvegard√©s (xgb_best_params.joblib)")

    return booster, ohe, ordinal_encoder, feature_names


# ============================================================================
# POINT D'ENTR√âE
# ============================================================================
# Si ce script est ex√©cut√© directement (pas import√©), lancer l'entra√Ænement
if __name__ == "__main__":
    train_model()
