================================================================================
                    PRÉSENTATION DU PROJET
         API DE PRÉDICTION D'ATTRITION - MACHINE LEARNING
================================================================================

Ce document sert de support de présentation pour le projet de déploiement
d'un modèle de Machine Learning. Il couvre tous les livrables demandés.

================================================================================
1. STRUCTURE DU DÉPÔT GIT ET PIPELINE CI/CD
================================================================================

1.1. STRUCTURE DU DÉPÔT GIT
----------------------------

Le dépôt est organisé selon les meilleures pratiques de développement :

```
docker_ml-fastapi/
├── .github/workflows/          # Configuration CI/CD
│   ├── ci.yml                  # Intégration Continue (tests, linting)
│   └── deploy-render.yml       # Déploiement Continu sur Render.com
├── app/                        # Code source de l'application
│   ├── main.py                 # API FastAPI principale
│   ├── model.py                # Chargement et utilisation du modèle XGBoost
│   ├── schemas.py              # Schémas Pydantic (validation)
│   ├── database.py             # Gestion PostgreSQL (pool de connexions)
│   ├── train_model.py          # Script d'entraînement du modèle
│   ├── settings.py             # Configuration de l'application
│   └── requirements.txt        # Dépendances Python
├── db/                         # Scripts de base de données
│   ├── db.py                   # Utilitaires SQLAlchemy
│   └── init/
│       ├── attrition.sql       # Script SQL avec données d'entraînement
│       └── prediction.sql      # Table des prédictions
├── tests/                      # Tests unitaires et fonctionnels
│   ├── test_api.py             # Tests des endpoints API
│   ├── test_database.py        # Tests de la base de données
│   ├── test_model.py           # Tests du modèle ML
│   └── test_train_model.py     # Tests du script d'entraînement
├── models/                     # Modèles entraînés et artefacts
│   ├── xgb_booster.json        # Modèle XGBoost (format JSON)
│   ├── onehot_encoder.joblib   # Encodeur OneHot (scikit-learn)
│   ├── ordinal_encoder.joblib  # Encodeur Ordinal (scikit-learn)
│   ├── feature_names.joblib    # Liste des features dans l'ordre
│   └── xgb_best_params.joblib  # Meilleurs hyperparamètres Optuna
├── Dockerfile                   # Image Docker pour l'API
├── docker-compose.yml           # Orchestration Docker (API + PostgreSQL)
├── pytest.ini                  # Configuration pytest
├── pyproject.toml               # Configuration du projet Python
└── README.md                    # Documentation principale
```

GESTION DES VERSIONS :
- Branches : `dev` (développement) et `main` (production)
- Tags : Utilisés pour marquer les versions (ex: v1.0.0)
- Commits : Messages clairs et descriptifs selon les conventions

1.2. PIPELINE CI/CD - INTÉGRATION CONTINUE
-------------------------------------------

FICHIER : .github/workflows/ci.yml

DÉCLENCHEURS :
- Push sur les branches `dev` et `main`
- Pull Request vers `dev` et `main`

JOB 1 : TEST
------------
Objectif : Exécuter les tests et générer un rapport de couverture

Étapes :
1. Checkout du code depuis GitHub
2. Installation de Python 3.11
3. Installation des dépendances (requirements.txt + pytest)
4. Démarrage d'un service PostgreSQL (conteneur Docker)
5. Initialisation de la base de données (script SQL)
6. Exécution des tests avec pytest
   - Tests unitaires et fonctionnels
   - Mesure de la couverture de code
   - Génération de rapports (XML pour Codecov, terminal)
7. Upload du rapport de couverture vers Codecov

JOB 2 : LINT
------------
Objectif : Vérifier la qualité et le formatage du code

Étapes :
1. Checkout du code
2. Installation de Python 3.11
3. Installation des outils de linting :
   - flake8 : vérification du style et des erreurs Python
   - black : vérification du formatage
   - isort : vérification du tri des imports
4. Exécution des vérifications :
   - flake8 : erreurs critiques (E9, F63, F7, F82)
   - flake8 : toutes les autres erreurs (avec seuils de complexité)
   - black : vérification du formatage
   - isort : vérification du tri des imports

RÉSULTAT :
- Si tous les tests passent ET le linting est OK → ✅ Succès
- Sinon → ❌ Échec (le workflow bloque la fusion)

1.3. PIPELINE CI/CD - DÉPLOIEMENT CONTINU
------------------------------------------

FICHIER : .github/workflows/deploy-render.yml

DÉCLENCHEURS :
- Push sur `dev` ou `main`
- Après la réussite du workflow CI (workflow_run)

ENVIRONNEMENTS :
- Branche `dev` → Environnement de développement (test)
- Branche `main` → Environnement de production

PROCESSUS :
1. Vérification que le workflow CI a réussi
2. Détermination de l'environnement (dev ou prod)
3. Envoi d'une requête POST au webhook Render.com
4. Render.com redéploie automatiquement l'application
5. Vérification de santé après déploiement (GET /health)

AVANTAGES :
- Déploiement automatique après validation des tests
- Séparation claire dev/prod
- Pas d'intervention manuelle nécessaire

================================================================================
2. API FASTAPI ET DOCUMENTATION SWAGGER/OPENAPI
================================================================================

2.1. FRAMEWORK : FASTAPI
------------------------

FastAPI est un framework Python moderne pour créer des APIs REST :
- Performance élevée (comparable à Node.js et Go)
- Validation automatique des données avec Pydantic
- Documentation automatique (Swagger/OpenAPI)
- Support natif des types Python (type hints)

2.2. ENDPOINTS DE L'API
------------------------

GET  /                    Page d'accueil avec informations sur l'API
GET  /health              Vérification de santé (modèle + base de données)
GET  /db-test             Test de connexion à PostgreSQL
POST /predict             Prédiction d'attrition pour un employé
GET  /predictions         Historique des prédictions (avec limite)
GET  /model-info          Informations techniques sur le modèle
GET  /docs                Documentation interactive Swagger UI
GET  /redoc               Documentation alternative ReDoc
POST /admin/train-model    Déclenchement de l'entraînement (avec authentification)

2.3. ENDPOINT PRINCIPAL : /predict
-----------------------------------

FONCTIONNALITÉ :
- Reçoit les données d'un employé (26 features)
- Valide automatiquement les données avec Pydantic
- Transforme les données selon le pipeline d'entraînement
- Fait une prédiction avec le modèle XGBoost
- Enregistre la prédiction en base de données
- Retourne la prédiction (0 ou 1) et la probabilité (0.0 à 1.0)

EXEMPLE DE REQUÊTE :
```json
POST /predict
{
  "employee_data": {
    "age": 35,
    "genre": "M",
    "revenu_mensuel": 5000,
    "statut_marital": "Marié(e)",
    "departement": "R&D",
    "poste": "Data Scientist",
    "nombre_experiences_precedentes": 3,
    "annee_experience_totale": 8,
    "annees_dans_l_entreprise": 5,
    "annees_dans_le_poste_actuel": 2,
    "satisfaction_employee_environnement": 4,
    "note_evaluation_precedente": 4.0,
    "niveau_hierarchique_poste": 2,
    "satisfaction_employee_nature_travail": 4,
    "satisfaction_employee_equipe": 4,
    "satisfaction_employee_equilibre_pro_perso": 3,
    "note_evaluation_actuelle": 4.5,
    "heure_supplementaires": "Non",
    "nombre_participation_pee": 2,
    "nb_formations_suivies": 3,
    "distance_domicile_travail": 10,
    "niveau_education": 3,
    "domaine_etude": "Data Science",
    "frequence_deplacement": "Occasionnel",
    "annees_depuis_la_derniere_promotion": 2,
    "annes_sous_responsable_actuel": 1,
    "augmentation_salaire_precedente": 0.05
  }
}
```

EXEMPLE DE RÉPONSE :
```json
{
  "prediction": 0,
  "probability": 0.23,
  "model_version": "1.0.0",
  "prediction_id": 42
}
```

2.4. DOCUMENTATION AUTOMATIQUE SWAGGER/OPENAPI
-----------------------------------------------

ACCÈS :
- Swagger UI : http://localhost:8090/docs
- ReDoc : http://localhost:8090/redoc

FONCTIONNALITÉS :
✅ Documentation automatique générée depuis le code
✅ Schémas de données (Pydantic) documentés automatiquement
✅ Exemples de requêtes et réponses
✅ Interface interactive pour tester les endpoints
✅ Validation en temps réel des données

AVANTAGES :
- Pas besoin de maintenir une documentation séparée
- Documentation toujours à jour (générée depuis le code)
- Interface utilisateur intuitive pour tester l'API
- Conformité OpenAPI 3.0 (standard industriel)

EXEMPLE DE DOCUMENTATION GÉNÉRÉE :
- Description de chaque endpoint
- Paramètres d'entrée avec types et contraintes
- Schémas de réponse avec exemples
- Codes d'erreur possibles (400, 422, 500, 503)
- Possibilité de tester directement depuis l'interface

2.5. VALIDATION DES DONNÉES AVEC PYDANTIC
------------------------------------------

Les schémas Pydantic garantissent :
- Validation automatique des types (int, float, str)
- Validation des contraintes (âge >= 18, probabilité entre 0 et 1)
- Messages d'erreur clairs en cas de données invalides
- Documentation automatique des schémas

EXEMPLE DE VALIDATION :
- Si l'âge est négatif → Erreur 422 (Unprocessable Entity)
- Si un champ obligatoire est manquant → Erreur 422
- Si le type est incorrect (string au lieu d'int) → Erreur 422

================================================================================
3. TESTS UNITAIRES ET FONCTIONNELS - PYTEST
================================================================================

3.1. STRUCTURE DES TESTS
-------------------------

FICHIERS DE TEST :
- tests/test_api.py          : Tests des endpoints API (47 tests)
- tests/test_database.py     : Tests de la base de données (10 tests)
- tests/test_model.py        : Tests du modèle ML (8 tests)
- tests/test_train_model.py  : Tests du script d'entraînement (6 tests)

TOTAL : 71 tests unitaires et fonctionnels

3.2. TYPES DE TESTS
-------------------

TESTS UNITAIRES :
- Testent des fonctions isolées (ex: safe_divide, _has_cols)
- Utilisent des mocks pour isoler les dépendances
- Rapides à exécuter (< 1 seconde)

TESTS FONCTIONNELS :
- Testent les endpoints API de bout en bout
- Utilisent TestClient de FastAPI (simule des requêtes HTTP)
- Vérifient les réponses JSON et les codes de statut

TESTS D'INTÉGRATION :
- Testent l'interaction avec la base de données (avec mocks)
- Vérifient le chargement et l'utilisation du modèle

3.3. EXEMPLES DE TESTS
----------------------

TEST D'ENDPOINT (test_api.py) :
```python
def test_predict_with_valid_data():
    """Test de prédiction avec des données valides"""
    payload = {"employee_data": SAMPLE_EMPLOYEE}
    res = client.post("/predict", json=payload)
    
    assert res.status_code in [200, 503]  # Accepte 503 si modèle non chargé
    if res.status_code == 200:
        data = res.json()
        assert "prediction" in data
        assert data["prediction"] in [0, 1]
        assert 0 <= data["probability"] <= 1
```

TEST DE VALIDATION (test_api.py) :
```python
def test_predict_validation_error_missing_field():
    """Test de validation avec champ manquant"""
    invalid_data = SAMPLE_EMPLOYEE.copy()
    del invalid_data["age"]  # Supprimer un champ obligatoire
    
    payload = {"employee_data": invalid_data}
    res = client.post("/predict", json=payload)
    
    assert res.status_code == 422  # Unprocessable Entity
```

TEST DE BASE DE DONNÉES (test_database.py) :
```python
def test_save_prediction_success():
    """Test de sauvegarde d'une prédiction"""
    # Utilise des mocks pour simuler PostgreSQL
    with patch('database.pool.SimpleConnectionPool'):
        db = Database("postgresql://...")
        prediction_id = db.save_prediction(
            input_data={"age": 35},
            prediction=1,
            probability=0.75,
            model_version="1.0.0"
        )
        assert prediction_id == 42
```

3.4. RAPPORT DE COUVERTURE DE TESTS
------------------------------------

COUVERTURE GLOBALE : 64%

DÉTAIL PAR FICHIER :
- app/__init__.py          : 100% (0 lignes, fichier vide)
- app/database.py          : 100% (57/57 lignes)
- app/schemas.py           : 100% (48/48 lignes)
- app/settings.py          : 100% (7/7 lignes)
- app/main.py              : 62%  (68/109 lignes, 41 manquantes)
- app/model.py             : 60%  (55/92 lignes, 37 manquantes)
- app/train_model.py       : 46%  (92/198 lignes, 106 manquantes)

TOTAL : 327/511 lignes couvertes (64%)

ANALYSE :
✅ Parties critiques bien testées :
   - Base de données (100%)
   - Validation des données (100%)
   - Endpoints API principaux (62%)

⚠️ Parties à améliorer :
   - Cas d'erreur dans les endpoints (branches except)
   - Transformation complète des données (model.py)
   - Script d'entraînement complet (train_model.py - acceptable car nécessite une vraie DB)

COMMENT AMÉLIORER :
- Ajouter des tests pour les cas d'erreur (exceptions)
- Tester _transform_employee_data() avec des données réelles
- Pour train_model.py : tests d'intégration optionnels (lents)

3.5. EXÉCUTION DES TESTS
------------------------

COMMANDES :
```bash
# Tous les tests
pytest tests/ -v

# Avec couverture
pytest tests/ --cov=app --cov-report=html

# Un fichier spécifique
pytest tests/test_api.py -v

# Un test spécifique
pytest tests/test_api.py::test_predict_with_valid_data -v
```

INTÉGRATION CI/CD :
- Les tests s'exécutent automatiquement à chaque push
- Le rapport de couverture est uploadé vers Codecov
- Le workflow échoue si les tests échouent

================================================================================
4. BASE DE DONNÉES POSTGRESQL
================================================================================

4.1. SCRIPT DE CRÉATION : db/init/attrition.sql
------------------------------------------------

Ce script SQL :
- Crée les tables nécessaires
- Insère les données d'entraînement
- Est exécuté automatiquement au démarrage du conteneur Docker

TABLES CRÉÉES :

1. TABLE : sirh (Système d'Information des Ressources Humaines)
   Colonnes principales :
   - id_employee (bigint, PRIMARY KEY)
   - age, genre, revenu_mensuel
   - statut_marital, departement, poste
   - nombre_experiences_precedentes
   - nombre_heures_travailless
   - annee_experience_totale
   - annees_dans_l_entreprise
   - annees_dans_le_poste_actuel

2. TABLE : eval (Évaluations de performance)
   Colonnes principales :
   - eval_number (varchar) → converti en id_employee
   - satisfaction_employee_environnement
   - note_evaluation_precedente, note_evaluation_actuelle
   - niveau_hierarchique_poste
   - satisfaction_employee_nature_travail
   - satisfaction_employee_equipe
   - satisfaction_employee_equilibre_pro_perso
   - heure_supplementaires
   - augementation_salaire_precedente

3. TABLE : sondage (Résultats de sondages)
   Colonnes principales :
   - code_sondage (varchar) → converti en id_employee
   - a_quitte_l_entreprise (variable cible : Oui/Non)
   - nombre_participation_pee
   - nb_formations_suivies
   - distance_domicile_travail
   - niveau_education, domaine_etude
   - frequence_deplacement
   - annees_depuis_la_derniere_promotion
   - annes_sous_responsable_actuel

4. TABLE : predictions (Prédictions enregistrées)
   Colonnes :
   - id (SERIAL, PRIMARY KEY)
   - input_data (JSONB) : données d'entrée de l'employé
   - prediction (INTEGER) : 0 ou 1
   - probability (FLOAT) : probabilité d'attrition
   - model_version (VARCHAR) : version du modèle utilisé
   - created_at (TIMESTAMP) : date de création

4.2. EXEMPLES D'ENTRÉES
------------------------

DONNÉES D'ENTRAÎNEMENT :
- 1470 employés dans la table `sirh`
- Données jointes avec `eval` et `sondage` sur `id_employee`
- Variable cible : `a_quitte_l_entreprise` (Oui = 1, Non = 0)

EXEMPLE D'EMPLOYÉ :
```sql
-- Table sirh
id_employee: 1
age: 35
genre: M
revenu_mensuel: 5000
departement: R&D
poste: Data Scientist
...

-- Table eval
eval_number: E_1
note_evaluation_actuelle: 4.5
satisfaction_employee_environnement: 4
...

-- Table sondage
code_sondage: 1
a_quitte_l_entreprise: Non
frequence_deplacement: Occasionnel
...
```

EXEMPLE DE PRÉDICTION ENREGISTRÉE :
```sql
INSERT INTO predictions (input_data, prediction, probability, model_version)
VALUES (
  '{"age": 35, "genre": "M", "revenu_mensuel": 5000, ...}',
  0,
  0.23,
  '1.0.0'
);
```

4.3. MODÈLE DE DONNÉES
----------------------

RELATIONS :
- `sirh.id_employee` → Clé primaire
- `eval.eval_number` → Converti en `id_employee` (ex: "E_1" → 1)
- `sondage.code_sondage` → Converti en `id_employee`
- Jointures LEFT JOIN pour combiner les 3 tables

SCHEMA UML SIMPLIFIÉ :
```
┌─────────────┐
│    sirh     │
│─────────────│
│ id_employee │──┐
│ age         │  │
│ genre       │  │
│ ...         │  │
└─────────────┘  │
                 │ LEFT JOIN
┌─────────────┐  │
│    eval     │  │
│─────────────│  │
│ eval_number │──┼──┐
│ note_eval   │  │  │
│ ...         │  │  │
└─────────────┘  │  │
                 │  │ LEFT JOIN
┌─────────────┐  │  │
│   sondage   │  │  │
│─────────────│  │  │
│code_sondage │──┘  │
│a_quitte...  │     │
│ ...         │     │
└─────────────┘     │
                    │
         ┌──────────┴──────────┐
         │   DataFrame final    │
         │   (pour entraînement)│
         └──────────────────────┘
```

GESTION DES DONNÉES :
- Pool de connexions PostgreSQL (1-5 connexions)
- Transactions automatiques (commit/rollback)
- Stockage JSONB pour les données d'entrée (flexible)

================================================================================
5. MODÈLE DE MACHINE LEARNING - CAS D'USAGE ET RÉSULTATS
================================================================================

5.1. ALGORITHME : XGBOOST
--------------------------

XGBoost (eXtreme Gradient Boosting) :
- Algorithme de gradient boosting très performant
- Idéal pour les données tabulaires
- Gère automatiquement les valeurs manquantes
- Classification binaire : l'employé reste (0) ou quitte (1)

OPTIMISATION DES HYPERPARAMÈTRES :
- Outil : Optuna (Tree-structured Parzen Estimator)
- Nombre d'essais : 60 (configurable via OPTUNA_TRIALS)
- Métrique optimisée : AUC-ROC (Area Under the ROC Curve)

5.2. PIPELINE D'ENTRAÎNEMENT
-----------------------------

ÉTAPE 1 : CHARGEMENT DES DONNÉES
- Lecture depuis PostgreSQL (3 tables : sirh, eval, sondage)
- Jointures LEFT JOIN sur id_employee
- Résultat : DataFrame pandas avec toutes les features

ÉTAPE 2 : PRÉPARATION DES DONNÉES
- Normalisation de augmentation_salaire_precedente (11% → 0.11)
- Création de la variable cible : Attrition (Oui=1, Non=0)
- Encodage OneHot pour les catégories nominales (département, poste, etc.)
- Encodage Ordinal pour frequence_deplacement
- Recodage heure_supplementaires (Oui/Non → 1/0)
- Création de features dérivées :
  * ratio_anciennete = annees_dans_l_entreprise / annee_experience_totale
  * ratio_poste = annees_dans_le_poste_actuel / annees_dans_l_entreprise
  * ecart_evaluation = note_evaluation_actuelle - note_evaluation_precedente
  * ratio_salaire_niveau = revenu_mensuel / niveau_hierarchique_poste
  * ratio_formations = nb_formations_suivies / annees_dans_l_entreprise
  * indice_recente_promo = 1 / (annees_depuis_la_derniere_promotion + 1)
- Suppression de colonnes corrélées (redondantes)
- Nettoyage final (NaN, valeurs infinies)

ÉTAPE 3 : OPTIMISATION OPTUNA
- Recherche des meilleurs hyperparamètres :
  * learning_rate (taux d'apprentissage)
  * max_depth (profondeur des arbres)
  * min_child_weight
  * subsample, colsample_bytree
  * reg_alpha, reg_lambda (régularisation)
- Early stopping pour éviter le surapprentissage
- Sélection du meilleur modèle selon AUC-ROC

ÉTAPE 4 : ENTRAÎNEMENT FINAL
- Réentraînement avec les meilleurs hyperparamètres
- Utilisation de best_iteration (early stopping)
- Division train/validation/test (80/10/10)

ÉTAPE 5 : ÉVALUATION
- Métriques calculées :
  * AUC-ROC (Area Under the ROC Curve)
  * AUC-PR (Area Under the Precision-Recall Curve)
  * Classification Report (précision, rappel, F1-score)

ÉTAPE 6 : SAUVEGARDE DES ARTEFACTS
- xgb_booster.json : modèle XGBoost entraîné
- onehot_encoder.joblib : encodeur OneHot (fit)
- ordinal_encoder.joblib : encodeur Ordinal (fit)
- feature_names.joblib : liste des features dans l'ordre
- xgb_best_params.joblib : meilleurs hyperparamètres Optuna

5.3. PIPELINE DE PRÉDICTION (EN PRODUCTION)
--------------------------------------------

LORS D'UNE REQUÊTE /predict :

1. RÉCEPTION DES DONNÉES
   - Validation automatique avec Pydantic
   - Vérification des types et contraintes

2. TRANSFORMATION DES DONNÉES
   - Conversion Pydantic → DataFrame pandas
   - Application du même pipeline qu'à l'entraînement :
     * Normalisation augmentation_salaire_precedente
     * Encodage OneHot (même encodeur qu'à l'entraînement)
     * Encodage Ordinal (même encodeur qu'à l'entraînement)
     * Recodage heure_supplementaires
     * Création des features dérivées
     * Suppression des colonnes corrélées
     * Nettoyage (NaN, infini)
     * Ajout des features manquantes (valeurs à 0)
     * Réordonnancement selon feature_names

3. PRÉDICTION
   - Création d'un DMatrix XGBoost
   - Prédiction avec le modèle (probabilité entre 0 et 1)
   - Conversion en prédiction binaire (seuil = 0.5)

4. ENREGISTREMENT
   - Sauvegarde en base de données (table predictions)
   - Traçabilité complète (input_data, prediction, probability, model_version)

5. RÉPONSE
   - Retour de la prédiction (0 ou 1) et de la probabilité

5.4. RÉSULTATS OBTENUS
----------------------

MÉTRIQUES D'ÉVALUATION (exemples typiques pour ce type de modèle) :

AUC-ROC : ~0.85-0.90
- Indique la capacité du modèle à distinguer les employés qui partent
- Valeur > 0.8 = bon modèle

AUC-PR : ~0.70-0.80
- Précision-rappel, utile pour les classes déséquilibrées
- Valeur > 0.7 = bon modèle

CLASSIFICATION REPORT :
- Précision (classe 1) : ~75-85%
- Rappel (classe 1) : ~60-70%
- F1-score (classe 1) : ~65-75%

INTERPRÉTATION :
- Le modèle identifie correctement la plupart des employés à risque
- Quelques faux positifs (employés prédits comme partants mais qui restent)
- Quelques faux négatifs (employés qui partent mais non détectés)

5.5. CAS D'USAGE
----------------

SCÉNARIO 1 : PRÉDICTION POUR UN NOUVEL EMPLOYÉ
-----------------------------------------------
1. RH saisit les données de l'employé dans l'interface
2. L'API reçoit les données via POST /predict
3. Le modèle prédit : probabilité = 0.23 → Prédiction = 0 (reste)
4. Action : Employé à faible risque, pas d'action immédiate

SCÉNARIO 2 : PRÉDICTION POUR UN EMPLOYÉ À RISQUE
-------------------------------------------------
1. Données d'un employé avec :
   - Faible satisfaction (2/5)
   - Pas de promotion depuis 5 ans
   - Heures supplémentaires fréquentes
2. Le modèle prédit : probabilité = 0.78 → Prédiction = 1 (quitte)
3. Action : Alerte RH, entretien préventif, analyse des causes

SCÉNARIO 3 : ANALYSE DE L'HISTORIQUE
------------------------------------
1. RH consulte GET /predictions?limit=100
2. Analyse des tendances :
   - % d'employés à risque par département
   - Évolution des prédictions dans le temps
   - Corrélation entre features et attrition
3. Action : Stratégies de rétention ciblées

SCÉNARIO 4 : RÉENTRAÎNEMENT DU MODÈLE
--------------------------------------
1. Nouvelles données collectées (6 mois plus tard)
2. Déclenchement via POST /admin/train-model
3. Optuna optimise les hyperparamètres sur les nouvelles données
4. Nouveau modèle déployé (version 1.1.0)
5. Amélioration continue des performances

5.6. FEATURES IMPORTANTES (IMPORTANCE DU MODÈLE)
-------------------------------------------------

Les features les plus importantes pour prédire l'attrition sont typiquement :
1. Satisfaction des employés (environnement, travail, équipe)
2. Années depuis la dernière promotion
3. Heures supplémentaires
4. Écart d'évaluation (actuelle vs précédente)
5. Ratio ancienneté / expérience totale
6. Revenu mensuel et niveau hiérarchique

INTERPRÉTATION :
- Les employés insatisfaits sont plus à risque
- Le manque de promotion est un facteur clé
- La surcharge de travail (heures sup) augmente le risque
- Les écarts d'évaluation négatifs sont préoccupants

5.7. LIMITES ET AMÉLIORATIONS POSSIBLES
---------------------------------------

LIMITES ACTUELLES :
- Modèle entraîné sur un dataset spécifique (1470 employés)
- Features statiques (pas de données temporelles)
- Pas de prise en compte des événements externes

AMÉLIORATIONS POSSIBLES :
- Ajout de données temporelles (historique des évaluations)
- Intégration de données externes (marché de l'emploi, économie)
- Modèles plus sophistiqués (deep learning pour séries temporelles)
- A/B testing pour valider les actions de rétention
- Dashboard de visualisation des prédictions

================================================================================
6. CONCLUSION
================================================================================

LIVRABLES RÉALISÉS :
✅ Dépôt Git structuré avec branches dev/main et tags
✅ Pipeline CI/CD complet (tests + linting + déploiement)
✅ API FastAPI fonctionnelle avec documentation Swagger/OpenAPI
✅ Tests unitaires et fonctionnels (71 tests, 64% de couverture)
✅ Base de données PostgreSQL avec scripts SQL et exemples
✅ Modèle XGBoost optimisé avec Optuna
✅ Déploiement automatique sur Render.com
✅ Documentation complète (README, guides, commentaires)

POINTS FORTS :
- Architecture propre et modulaire
- Tests automatisés intégrés au CI/CD
- Documentation automatique de l'API
- Pipeline de ML complet (entraînement → déploiement → prédiction)
- Traçabilité complète (prédictions enregistrées en DB)

PERSPECTIVES :
- Améliorer la couverture de tests (70-80%)
- Ajouter des tests d'intégration
- Créer un dashboard de visualisation
- Implémenter un système de monitoring des prédictions

================================================================================
                        FIN DE LA PRÉSENTATION
================================================================================
